====== Unsupervised learning ======
{{anchor:tutorials.unsupervised}}

In this tutorial, we're going to learn how to define an autoencoder, and train
it using an unsupervised approach.

All the examples in this tutorial rely on the ''unsup'' and ''optim'' packages. 
If you want to visualize the training data, and filters (which is really important), 
the ''image'' package should also be installed. Also, if you're reading this online,
you should install the ''tutorials'' package, which provides the code
for these examples.As for all torch packages, they can be installed like this (at the 
command line):

<file bash>
$ torch-pkg install tutorials unsup optim image
</file>

All the examples in this section are based on:
''PREFIX/share/torch/lua/tutorials/train-autoencoder.lua''
If you are comfortable enough with autoencoders, and Torch, you might
want to jump right into the code.

===== Basic Autoencoder =====

An autoencoder is a model that takes a vector input ''y'', maps it into
a hidden representation ''z'' (code) using an encoder which typically has 
this form:

{{auto_encoder.png}}

where ''s'' is a non-linear activation function (the ''tanh'' function
is a common choice), ''W_e'' the encoding matrix and ''b_e'' a vector
of bias parameters.

The hidden representation ''z'', often called code, is then mapped back
into the space of ''y'', using a decoder of this form:

{{auto_decoder.png}}

where ''W_d'' is the decoding matrix and ''b_d'' a vector of bias
parameters.

The goal of the autoencoder is to minimize the reconstruction error,
which is represented by a distance between ''y'' and ''y~''. The
most common type of distance is the mean squared error:

{{mse_loss.png}}

The code ''z'' typically has less dimensions than ''y'', which forces
the autoencoder to learn a good representation of the data. In its 
simplest form (linear), an autoencoder learns to project the data
onto its first principal components. If the code ''z'' has as many
components as ''y'', then no compression is required, and the model
could typically end up learning the identity function. Now if
the encoder has a non-linear form (using a ''tanh'', or using a 
multi-layered model), then the autoencoder can learn a potentially
more powerful representation of the data.

==== Model description ====

To describe the model, we use the ''unsup'' package, which provides
templates to build autoencoders. 

The first step is to describe an encoder, which we can do by using
any of the modules available in [[..:nn:index|nn]]:

<file lua>
encoder = nn.Sequential()
encoder:add(nn.Linear(inputSize,outputSize))
encoder:add(nn.Tanh())
</file>

The second step is to describe the decoder, a simple linear module:

<file lua>
decoder = nn.Sequential()
decoder:add(nn.Linear(outputSize,inputSize))
</file>

Finally, we use the built-in AutoEncoder class from unsup, which
automatically provides a mean-square error loss:

<file lua>
module = unsup.AutoEncoder(encoder, decoder, params.beta)
</file>

At this stage, estimating the loss (reconstruction error) can be
done like this, for arbitrary inputs:

<file lua>
input = torch.randn(inputSize)
loss = module:updateOutput(input,input)
</file>

Note that we need to provide the input, and a target that we wish
to reconstruct. In this case the target is the input, but in some
cases, we might want to provide a noisy version of the input, and
force the autoencoder to predict the correct input (this is what
denoising autoencoders do).

As for any ''nn'' module, gradients can be estimated this way:

<file lua>
-- get parameters and gradient pointers
x,dl_dx = module:getParameters()

-- compute loss
loss = module:updateOutput(inputs[i], targets[i])

-- compute gradients wrt input and weights
dl_dx:zero()
module:updateGradInput(input, input)
module:accGradParameters(input, input)

-- at this stage, dl_dx contains the gradients of the loss wrt
-- the trainable parameters x
</file>

==== Training ====

If you've read the tutorial on supervised learning, training a model
unsupervised is basically equivalent. We first define a closure
that computes the loss, and the gradients of that loss wrt the trainable
parameters, and then pass this closure to one of the optimizers in
''optim''. As usual, we use SGD to train autoencoders on large
amounts of data:

<file lua>
-- some parameters
local minibatchsize = 50

-- parameters
x,dl_dx = module:getParameters()

-- SGD config
sgdconf = {learningRate = 1e-3}

-- assuming a table trainData with the form:
-- trainData = {
--	  [1] = sample1,
--    [2] = sample2,
--    [3] ...
-- }
for i = 1,#trainData,minibatchsize do

	-- create minibatch of training samples
	samples = torch.Tensor(minibatchsize,inputSize)
	for i = 1,minibatchsize do
		samples[i] = trainData[i]
	end

	-- define closure
	local feval = function()
      -- reset gradient/f
      local f = 0
      dl_dx:zero()

      -- estimate f and gradients, for minibatch
      for i = 1,minibatchsize do
         -- f
         f = f + module:updateOutput(samples[i], samples[i])

         -- gradients
         module:updateGradInput(samples[i], samples[i])
         module:accGradParameters(samples[i], samples[i])
      end

      -- normalize
      dl_dx:div(minibatchsize)
      f = f/minibatchsize

      -- return f and df/dx
      return f,dl_dx
   end

   -- do SGD step
   optim.sgd(feval, x, sgdconf)

end
</file>

Ok, that's it, given some training data, this code will loop
over all samples, and minimize the reconstruction error, using
stochastic gradient descent.

One big shortcoming of basic autoencoders is that it's usually
hard to train them, and hard to avoid getting to close to learning
the identity function. In practice, using a code ''y'' that is smaller
than ''x'' is enough to avoid learning the identity, but it remains
hard to do much better than PCA. 


Using codes that are overcomplete
(i.e. with more components than the input) makes the problem even
worse. There are different ways that an autoencoder with an overcomplete
code may still discover interesting representations. One common way
is the addition of sparsity: by forcing units of the hidden representation
to be mostly 0s, the autoencoder has to learn a nice distributed representation
of the data.

In the following section, we present a method to impose sparsity on the code, 
which typically allows codes that are overcomplete, sparse, and very useful for
tasks like classification/recognition.

===== Predictive Sparse Decomposition (PSD) Autoencoder =====

Adaptive sparse coding methods learn a possibly overcomplete set of basis 
functions, such that natural image patches can be reconstructed by linearly 
combining a small subset of these bases. The applicability of these methods 
to visual object recognition tasks has been limited because of the prohibitive
cost of the optimization algorithms required to compute the sparse representation. 

In this tutorial we propose a simple and efficient algorithm to learn overcomplete 
basis functions, by introducing a particular form of autoencoder. After training, 
the model also provides a fast and smooth approximator to the optimal representation,
achieving even better accuracy than exact sparse coding algorithms on visual object
recognition tasks.

==== Sparse Coding ====

Finding a representation ''z'' in ''R^m'' for a given signal ''y'' in ''R^n'' 
by linear combination of an overcomplete set of basis vectors, columns 
of matrix ''B'' with m > n, has infinitely many solutions. In
optimal sparse coding, the problem is formulated as:

{{sparse_coding.png}}

where the l0 norm is defined as the number of non-zero elements in a given 
vector. This is a combinatorial problem, and a common approximation of it
is the following optimization problem:

{{sparse_coding_optim.png}}

This particular formulation, called Basis Pursuit Denoising, can be seen as 
minimizing an objective that penalizes the reconstruction error using a 
linear basis set and the sparsity of the corresponding representation. While
this formulation is nice, inference requires running some sort of
iterative minimization algorithm that is always computationally 
expensive. In the following we present a predictive version of this algorithm,
based on an autoencoder formulation, which yields fixed-time, and 
fast inference.

==== Linear PSD ====

In order to make inference efficient, we train a non-linear regressor that 
maps input signals ''y'' to sparse representations ''z''. We consider the 
following nonlinear mapping:

{{psd_encoder.png}}

where ''W'' is a weight trainable matrix, ''d'' a trainable vector of 
biases, and ''g'' a vector of gains. We want to train this nonlinear mapping
as a predictor for the optimal solution to the sparse coding algorithm
presented in the previsous section.

The following loss function, called predictive sparse decomposition, can
help us achieve such a goal:

{{psd_loss.png}}

The first two terms are the basic sparse coding presented above, while the
3rd term is our predictive sparse regressor. Minimizing this loss yields
an encoder that produces sparse decompositions of the input signal.

With the ''unsup'' package, this can be implemented very simply. 

We define an encoder first:

<file lua>
encoder = nn.Sequential()
encoder:add(nn.Linear(inputSize,outputSize))
encoder:add(nn.Tanh())
encoder:add(nn.Diag(outputSize))
</file>

Then the decoder is the L1 solution presented above:

<file lua>
decoder = unsup.LinearFistaL1(inputSize, outputSize, params.lambda)
</file>

Under the hood, this decoder relies on FISTA to find the optimal sparse
code. FISTA is available in the ''optim'' package.

Finally, both modules can be packaged together into an autoencoder. We
can't use the basic AutoEncoder class to do this, because the 
LinearFistaL1 decoder is a bit peculiar. Insted, we use a special-purpose
PSD container:

<file lua>
module = unsup.PSD(encoder, decoder)
</file>

==== Convolutional PSD ====

For vision/image applications, fully connected linear autoencoders
are often overkill, in their number of trainable parameters. Using
convolutional filters, inspired by convolutional networks (see supervised
learning tutorial on ConvNets) can help learn much better filters for
vision.

A convolutional version of the PSD autoencoder can be derived by simply
replacing the encoder and decoder by convolutional counterparts:

<file lua>
-- connection table:
conntable = nn.tables.full(1, 32)

-- decoder's table:
local decodertable = conntable:clone()
decodertable[{ {},1 }] = conntable[{ {},2 }]
decodertable[{ {},2 }] = conntable[{ {},1 }]
local outputSize = conntable[{ {},2 }]:max()

-- encoder:
encoder = nn.Sequential()
encoder:add(nn.SpatialConvolutionMap(conntable, 5, 5))
encoder:add(nn.Tanh())
encoder:add(nn.Diag(outputSize))

-- decoder is L1 solution:
decoder = unsup.SpatialConvFistaL1(decodertable, 5, 5, 25, 25)
</file>

==== Training ====

Training is done with the exact same procedure as presented above, for
the basic autoencoder.
















